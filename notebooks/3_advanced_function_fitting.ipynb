{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dda14e1-e1e1-4b94-8eec-83da75c532f3",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tee-lab/PyDaddy/blob/colab/notebooks/3_advanced_function_fitting.ipynb)\n",
    "\n",
    "# Advanced Function Fitting\n",
    "\n",
    "(This notebook assumes that you have gone through the [Getting Started](./1_getting_started.ipynb) notebook. The explanations and examples in the notebook will mostly be based on vector datasets, but the ideas remains mostly unchanged for the scalar case.)\n",
    "\n",
    "The _Getting Started_ notebook introduced how to fit functional forms for drift and diffusion terms. In those examples, the automatic model selection worked well. However, in many cases, some amount of manual intervention may be required to get the best results. \n",
    "\n",
    "There are two key parameters to (polynomial) function fitting; the _maximum degree_ of the polynomial, and the _sparsification threshold_. This notebook explores how to choose these parameters appropriately. \n",
    "\n",
    "As an experimental feature, `pydaddy` also allows us to fit functions other than polynomials, by providing a custom library. For more details on this, see [Fitting Non-polynomial Functions](./6_non_poly_function_fitting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d9fe9-d90e-4f5e-82f9-abdd96ded887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydaddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd9d44-605b-4202-97dd-e0974f5e80da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choosing the correct polynomial degree\n",
    "\n",
    "For this notebook, we will use the group polarization time-series for a school of fish. Assuming there are no preferred directions for the school, we can expect symmetries between $F_1$ and $F_2$ and between $G_{11}$ and $G_{22}$.\n",
    "\n",
    "The first step is to choose the right order for the polynomial. This can be done by visually inspecting the drift and diffusion plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de238046-82ab-4a64-aac8-e33b7af6eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, t = pydaddy.load_sample_dataset('model-data-vector-ternary')\n",
    "ddsde = pydaddy.Characterize(data, t, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e5bc4-3d77-4369-a9c5-8794deb79139",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d77417-7584-46ba-b455-0e0f6974dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fad30-86f5-4d38-953b-053368e60f5e",
   "metadata": {},
   "source": [
    "From the drift and diffusion plots, it looks like the the diffusion is quadratic. The drift looks cubic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b179a1d-3322-494e-a5e5-67e65292295b",
   "metadata": {},
   "source": [
    "## Setting the sparsification threshold\n",
    "\n",
    "The automatic model selection algorithm of `pydaddy` is a good place to start, and usually works well. This can be done by calling `ddsde.fit()` with argument `tune=True`.\n",
    "\n",
    "The model selection algorithm estimates polynomial fits for a wide range of threshold values; compute the _cross validation error_ for each model, and find the model that achieves the best drop in cross validation error. By default, range of thresholds to search over is automatically determined, but this can be specified manually if required, using the `thresholds` parameter: see the [documentation](http://pydaddy.readthedocs.io/api.html#pydaddy.daddy.Daddy.fit) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e1bbd-f8d9-453d-8445-680f8f3c15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = ddsde.fit('F1', order=3, tune=True)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605069a2-21df-45ea-8539-901c64e71f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = ddsde.fit('F2', order=3, tune=True)\n",
    "print(F2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19f3c9-acd2-4767-b3f1-c79ca9b37978",
   "metadata": {},
   "outputs": [],
   "source": [
    "G11 = ddsde.fit('G11', order=2, tune=True)\n",
    "print(G11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17a2f8-28c7-429c-b6f1-ab9c65158dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "G22 = ddsde.fit('G22', order=2, tune=True)\n",
    "print(G22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b666e58-0412-46ba-9dca-62821e60dbe0",
   "metadata": {},
   "source": [
    "## Evaluting and fine-tuning fits\n",
    "\n",
    "The goodness of the fit can be examined qualitatively by visualizing the fit against the points using `ddsde.drift()` or `ddsde.diffusion()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3defef-96d9-4186-885b-e3f26f0f5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a40aaa-93b2-4bc7-8d7a-2401e4dd81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99787f6c-d17a-493d-bb00-37276285bd46",
   "metadata": {},
   "source": [
    "For this example, auotmatic model selection (with the correct degree) worked well for $F_1$, $F_2$ and $G_{11}$, but failed for $G_{22}$. The automatic model selection can sometime fail like this: it may eliminate too many terms or return a polynomial with many spurious terms. In these cases, some manual intervention can improve results.\n",
    "\n",
    "The sparse regression algorithm used for fitting uses a sparsity threshold, which can be either manually specified, or be automatically tuned for (as above). The sparsity threshold can be interpreted as follows: for a given value of the sparsity threshold, the fitted polynomial wil only have coefficients with magnitudes higher than the threshold value. Therefore, if you set the threshold too high, there most or all terms will be zeroed out. If the threshold is too low, the result will have too many terms.\n",
    "\n",
    "When there are spurious terms present, one intuitive approach is to look for terms that are clearly spurious; i.e. some terms with coefficients an order of magnitude lesser than others. We can eliminate these terms by setting a threshold high enough to kill off these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e8e4e-28ae-4408-8674-e4d066dfb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold too small\n",
    "print(ddsde.fit('F1', order=3, threshold=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518ad3f-8f65-4185-b741-d7a98fb8b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold too high\n",
    "print(ddsde.fit('F1', order=3, threshold=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda771ae-6dc9-403f-b5b9-b5779c948695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold just right\n",
    "print(ddsde.fit('F1', order=3, threshold=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202791d-6cec-4f7f-8796-f283a05de0d6",
   "metadata": {},
   "source": [
    "A more systematic approach would be to look at the cross-validation curve, to try and identify if there is a clearly suitable threshold value where the CV error is either lowest, or has a significant jump. To this, use call `ddsde.fit()` with `tune=True` and `plot=True`. This will plot the cross validation error and the level of sparsity (i.e. the number of terms) for each level of threshold, helping you choose the right threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16e788-a227-4a0a-8dec-31f9d13ffaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.fit('F1', order=3, tune=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ab382-d6ef-453a-9d32-bd5f11bc266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.fit('G11', order=3, tune=True, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929d1b1-0945-40f5-b145-5e25405605ac",
   "metadata": {},
   "source": [
    "In the above example, for $A_1$, thresholds between ~0.02 and ~0.06 seems to give the best trade-off, with 3 terms. Decreasing the threshold further drastically increases the number of terms, with no significant improvement in CV error. Increasing the threshold above 0.6 kills off all the terms and makes the CV error very high.\n",
    "\n",
    "Similarly, for $B_{11}$, thresholds between ~0.01 and ~0.04 achieve the best trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708ee93-d59d-4b49-8ced-42e452116967",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddsde.fit('F1', order=3, threshold=0.03))\n",
    "print(ddsde.fit('G11', order=2, threshold=0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32279a4e",
   "metadata": {},
   "source": [
    "## Handling outliers using regularized regression\n",
    "\n",
    "The `fit()` function has an `alpha` parameter which is a ridge regularization parameter for the polynomial fitting. When a non-zero value of alpha is given, ridge regression is used in the fitting process. This will give best results when the data is noisy or has outliers. \n",
    "\n",
    "In cases when a non-zero `alpha` parameter is required, fairly high values of alpha may be required: as a rough rule of thumb, try `alpha=100` to `alpha=1e6` (this is a very vague rule of thumb; your mileage may vary according to your specific dataset). However, be aware of the fact that alpha has the overall effect of shrinking your model parameters: if alpha is too high, your estimated coefficients can be biased to be too small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a268fb9",
   "metadata": {},
   "source": [
    "As en example, see how fitting the diffusion function with a very large degree gives rise to overfitting, even when the threshold is fairly high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.fit('G11', order=10, threshold=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a652cd3",
   "metadata": {},
   "source": [
    "Setting `alpha` to be fairly high and re-doing the regression eliminates the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsde.fit('G11', order=10, threshold=0.02, alpha=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f6ec8",
   "metadata": {},
   "source": [
    "Notice that the coefficients from the ridge-regression-based fitting is slightly smaller than the coefficients we obtained during our earlier fits: this is the side effect of ridge regularization. Therefore, non-zero alpha should be only used as a last resort when other attempts fail.\n",
    "\n",
    "For more information on ridge regression, see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Ridge_regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge_regression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}